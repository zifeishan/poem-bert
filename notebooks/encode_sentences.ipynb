{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from bert import optimization\n",
    "from bert import modeling\n",
    "from bert import tokenization\n",
    "\n",
    "bert_paths = {\n",
    "    \"rbtl3\": \"/Users/zifei/Downloads/chinese-bert/chinese_rbtl3_L-3_H-1024_A-16\",\n",
    "    \"rbt-ext\": \"/Users/zifei/Downloads/chinese-bert/chinese_roberta_wwm_ext_L-12_H-768_A-12\",\n",
    "}\n",
    "bert_dir = bert_paths[\"rbtl3\"]\n",
    "tokenizer = tokenization.FullTokenizer(bert_dir + \"/vocab.txt\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zifei/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '白鹿今何在？']\n"
     ]
    }
   ],
   "source": [
    "for line in open(\"sents_all.tsv\"):\n",
    "    parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    sid, pid, sent = parts\n",
    "    print(parts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bert.modeling.BertConfig at 0x109749390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(bert_dir + \"/bert_config_rbtl3.json\")\n",
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor 'Placeholder:0' shape=(?, 16) dtype=int32>,\n",
       " 'input_mask': <tf.Tensor 'Placeholder_1:0' shape=(?, 16) dtype=int32>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_placeholders(batch_size=None, seq_length=16):\n",
    "    features = {\n",
    "        \"input_ids\": tf.placeholder(tf.int32, [batch_size, seq_length]),\n",
    "        \"input_mask\": tf.placeholder(tf.int32, [batch_size, seq_length]),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "features = create_placeholders()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zifei/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From ../bert/modeling.py:674: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/zifei/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "# CREATE BERT MODEL\n",
    "\n",
    "model = modeling.BertModel(\n",
    "  config=bert_config,\n",
    "  is_training=False,\n",
    "  input_ids=features[\"input_ids\"],\n",
    "  input_mask=features[\"input_mask\"],\n",
    "  token_type_ids=None,\n",
    "  use_one_hot_embeddings=False)\n",
    "\n",
    "pooled_output = model.get_pooled_output()\n",
    "# TODO: add an output embedding layer when training the model.\n",
    "\n",
    "# Are the output embeddings semantically meaningful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bert/pooler/dense/Tanh:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "  name = bert/embeddings/word_embeddings:0, shape = (21128, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/embeddings/token_type_embeddings:0, shape = (2, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/embeddings/position_embeddings:0, shape = (512, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/embeddings/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/embeddings/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
      "  name = bert/pooler/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
      "  name = bert/pooler/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n"
     ]
    }
   ],
   "source": [
    "init_checkpoint = bert_dir + '/bert_model.ckpt'\n",
    "use_tpu = False\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "scaffold_fn = None\n",
    "(assignment_map,\n",
    " initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "     tvars, init_checkpoint)\n",
    "\n",
    "# HACK for keras LayerNorm\n",
    "for k in list(assignment_map.keys()):\n",
    "    if k.endswith(\"/gamma\"):\n",
    "        del assignment_map[k]\n",
    "    if k.endswith(\"/beta\"):\n",
    "        assignment_map[k[:-4]] = k[:-4]\n",
    "        del assignment_map[k]\n",
    "        \n",
    "if use_tpu:\n",
    "  def tpu_scaffold():\n",
    "    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "    return tf.train.Scaffold()\n",
    "  scaffold_fn = tpu_scaffold\n",
    "else:\n",
    "  tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "tf.logging.info(\"**** Trainable Variables ****\")\n",
    "for var in tvars:\n",
    "  init_string = \"\"\n",
    "  if var.name in initialized_variable_names:\n",
    "    init_string = \", *INIT_FROM_CKPT*\"\n",
    "  print(\"  name = %s, shape = %s%s\" % (var.name, var.shape, init_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1' has no attribute 'global_variable_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6fc21edd1385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variable_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1' has no attribute 'global_variable_initializer'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[101, 872, 1962, 686, 4518, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 872, 1962, 686, 4518, 872, 1962, 686, 4518, 872, 1962, 686, 4518, 872, 1962, 686]], [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sents_batch, seq_len=16):\n",
    "    batch_ids = []\n",
    "    batch_masks = []\n",
    "    for s in sents_batch:\n",
    "        tokens = [\"[CLS]\"] + tokenizer.tokenize(s) + [\"[SEP]\"]\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        ids = ids[:seq_len]\n",
    "        ids = ids + [0] * (seq_len - len(ids))\n",
    "        masks = [int(i>0) for i in ids]\n",
    "        batch_ids.append(ids)\n",
    "        batch_masks.append(masks)\n",
    "    return batch_ids, batch_masks\n",
    "\n",
    "print(tokenize([\"你好世界\", \"你好世界你好世界你好世界你好世界你好世界\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids, test_masks = tokenize([\"你好世界\", \"你好世界你好世界你好世界你好世界你好世界\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80590755 -0.04846638  0.41607088 ...  0.0404962  -0.9999986\n",
      " -0.6127925 ]\n",
      "[-0.17146362  0.16745898  0.3858084  ...  0.3777602  -0.99995214\n",
      " -0.33941993]\n",
      "CPU times: user 25.3 s, sys: 2.2 s, total: 27.5 s\n",
      "Wall time: 8.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embs = sess.run(pooled_output, feed_dict={\n",
    "    features[\"input_ids\"]: test_ids * 256,\n",
    "    features[\"input_mask\"]: test_masks * 256,\n",
    "})\n",
    "print(embs[0])\n",
    "print(embs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing sentence 1\n",
      "processing sentence 10001\n",
      "processing sentence 20001\n",
      "processing sentence 30001\n",
      "processing sentence 40001\n",
      "processing sentence 50001\n",
      "processing sentence 60001\n",
      "processing sentence 70001\n",
      "processing sentence 80001\n",
      "processing sentence 90001\n",
      "processing sentence 100001\n",
      "processing sentence 110001\n",
      "processing sentence 120001\n",
      "processing sentence 130001\n",
      "processing sentence 140001\n",
      "processing sentence 150001\n",
      "processing sentence 160001\n",
      "processing sentence 170001\n",
      "processing sentence 180001\n",
      "processing sentence 190001\n",
      "processing sentence 200001\n",
      "processing sentence 210001\n",
      "processing sentence 220001\n",
      "processing sentence 230001\n",
      "processing sentence 240001\n",
      "processing sentence 250001\n",
      "processing sentence 260001\n",
      "processing sentence 270001\n",
      "processing sentence 280001\n",
      "processing sentence 290001\n",
      "processing sentence 300001\n",
      "processing sentence 310001\n",
      "processing sentence 320001\n",
      "processing sentence 330001\n",
      "processing sentence 340001\n",
      "processing sentence 350001\n",
      "processing sentence 360001\n",
      "processing sentence 370001\n",
      "processing sentence 380001\n",
      "processing sentence 390001\n",
      "processing sentence 400001\n",
      "processing sentence 410001\n",
      "processing sentence 420001\n",
      "processing sentence 430001\n",
      "processing sentence 440001\n",
      "processing sentence 450001\n",
      "processing sentence 460001\n",
      "processing sentence 470001\n",
      "processing sentence 480001\n",
      "processing sentence 490001\n",
      "processing sentence 500001\n",
      "processing sentence 510001\n",
      "processing sentence 520001\n",
      "processing sentence 530001\n",
      "processing sentence 540001\n",
      "processing sentence 550001\n",
      "processing sentence 560001\n",
      "processing sentence 570001\n",
      "processing sentence 580001\n",
      "processing sentence 590001\n",
      "processing sentence 600001\n",
      "processing sentence 610001\n",
      "processing sentence 620001\n",
      "processing sentence 630001\n",
      "processing sentence 640001\n",
      "processing sentence 650001\n",
      "processing sentence 660001\n",
      "processing sentence 670001\n",
      "processing sentence 680001\n",
      "processing sentence 690001\n",
      "processing sentence 700001\n",
      "processing sentence 710001\n",
      "processing sentence 720001\n",
      "processing sentence 730001\n",
      "processing sentence 740001\n",
      "processing sentence 750001\n",
      "processing sentence 760001\n",
      "processing sentence 770001\n",
      "processing sentence 780001\n",
      "processing sentence 790001\n",
      "processing sentence 800001\n",
      "processing sentence 810001\n",
      "processing sentence 820001\n",
      "processing sentence 830001\n",
      "processing sentence 840001\n",
      "processing sentence 850001\n",
      "processing sentence 860001\n",
      "processing sentence 870001\n",
      "processing sentence 880001\n",
      "processing sentence 890001\n",
      "processing sentence 900001\n",
      "processing sentence 910001\n",
      "processing sentence 920001\n",
      "processing sentence 930001\n",
      "processing sentence 940001\n",
      "processing sentence 950001\n",
      "processing sentence 960001\n",
      "processing sentence 970001\n",
      "processing sentence 980001\n",
      "processing sentence 990001\n",
      "processing sentence 1000001\n",
      "processing sentence 1010001\n",
      "processing sentence 1020001\n",
      "processing sentence 1030001\n",
      "processing sentence 1040001\n",
      "processing sentence 1050001\n",
      "processing sentence 1060001\n",
      "processing sentence 1070001\n",
      "processing sentence 1080001\n",
      "processing sentence 1090001\n",
      "processing sentence 1100001\n",
      "processing sentence 1110001\n",
      "processing sentence 1120001\n",
      "processing sentence 1130001\n",
      "processing sentence 1140001\n",
      "processing sentence 1150001\n",
      "processing sentence 1160001\n",
      "processing sentence 1170001\n",
      "processing sentence 1180001\n",
      "processing sentence 1190001\n",
      "processing sentence 1200001\n",
      "processing sentence 1210001\n",
      "processing sentence 1220001\n",
      "processing sentence 1230001\n",
      "processing sentence 1240001\n",
      "processing sentence 1250001\n",
      "processing sentence 1260001\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE ALL SENTENCE ENCODINGS. They are 1024 dimensional.\n",
    "import time\n",
    "import gzip\n",
    "\n",
    "batch_size = 128  # larger may be faster on TPU but not necessarily on CPU\n",
    "seq_len = 16\n",
    "batch = []\n",
    "\n",
    "OUTPUT_DIR = '/Volumes/WDMPBlue/Files/chinese-bert/poem-data/'\n",
    "def encode_and_write(batch, fout):\n",
    "    sents = [b[1] for b in batch]\n",
    "    sids = [b[0] for b in batch]\n",
    "    ids, masks = tokenize(sents, seq_len=seq_len)\n",
    "    embs = sess.run(pooled_output, feed_dict={\n",
    "        features[\"input_ids\"]: ids,\n",
    "        features[\"input_mask\"]: masks,\n",
    "    })\n",
    "    assert len(sids) == len(embs)\n",
    "    for sid, emb in zip(sids, embs):\n",
    "        # Need to write bytes\n",
    "        # gzfout.write(\"{}\\t{}\\n\".format(sid, \",\".join([str(e) for e in emb])).encode(\"utf-8\"))\n",
    "        fout.write(\"{}\\t{}\\n\".format(sid, \",\".join([str(e) for e in emb])))\n",
    "\n",
    "    \n",
    "# with gzip.open(OUTPUT_DIR + 'sent_encodings_all.tsv.gz', 'w') as gzfout:  # bottleneck\n",
    "\n",
    "with open(OUTPUT_DIR + 'sent_encodings_all.tsv', 'w') as fout:\n",
    "    for line in open(\"sents_all.tsv\"):\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        sid, pid, sent = parts\n",
    "        if int(sid) % 10000 == 1:\n",
    "            print(\"processing sentence {}\".format(sid))\n",
    "        if len(batch) == batch_size:\n",
    "            encode_and_write(batch, fout)\n",
    "            batch = []\n",
    "        else:\n",
    "            batch.append((sid, sent))\n",
    "\n",
    "    if batch:  # last batch\n",
    "        encode_and_write(batch, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('/Volumes/WDMPBlue/Files/chinese-bert/poem-data/test.tsv.gz', 'wb') as fout:\n",
    "    fout.write(b\"Hello\\n\")\n",
    "    #fout.write(u\"Hello\")\n",
    "    #fout.write(b\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bert/embeddings/position_embeddings',\n",
       "              'bert/embeddings/position_embeddings'),\n",
       "             ('bert/embeddings/token_type_embeddings',\n",
       "              'bert/embeddings/token_type_embeddings'),\n",
       "             ('bert/embeddings/word_embeddings',\n",
       "              'bert/embeddings/word_embeddings'),\n",
       "             ('bert/encoder/layer_0/attention/output/dense/bias',\n",
       "              'bert/encoder/layer_0/attention/output/dense/bias'),\n",
       "             ('bert/encoder/layer_0/attention/output/dense/kernel',\n",
       "              'bert/encoder/layer_0/attention/output/dense/kernel'),\n",
       "             ('bert/encoder/layer_0/attention/self/key/bias',\n",
       "              'bert/encoder/layer_0/attention/self/key/bias'),\n",
       "             ('bert/encoder/layer_0/attention/self/key/kernel',\n",
       "              'bert/encoder/layer_0/attention/self/key/kernel'),\n",
       "             ('bert/encoder/layer_0/attention/self/query/bias',\n",
       "              'bert/encoder/layer_0/attention/self/query/bias'),\n",
       "             ('bert/encoder/layer_0/attention/self/query/kernel',\n",
       "              'bert/encoder/layer_0/attention/self/query/kernel'),\n",
       "             ('bert/encoder/layer_0/attention/self/value/bias',\n",
       "              'bert/encoder/layer_0/attention/self/value/bias'),\n",
       "             ('bert/encoder/layer_0/attention/self/value/kernel',\n",
       "              'bert/encoder/layer_0/attention/self/value/kernel'),\n",
       "             ('bert/encoder/layer_0/intermediate/dense/bias',\n",
       "              'bert/encoder/layer_0/intermediate/dense/bias'),\n",
       "             ('bert/encoder/layer_0/intermediate/dense/kernel',\n",
       "              'bert/encoder/layer_0/intermediate/dense/kernel'),\n",
       "             ('bert/encoder/layer_0/output/dense/bias',\n",
       "              'bert/encoder/layer_0/output/dense/bias'),\n",
       "             ('bert/encoder/layer_0/output/dense/kernel',\n",
       "              'bert/encoder/layer_0/output/dense/kernel'),\n",
       "             ('bert/encoder/layer_1/attention/output/dense/bias',\n",
       "              'bert/encoder/layer_1/attention/output/dense/bias'),\n",
       "             ('bert/encoder/layer_1/attention/output/dense/kernel',\n",
       "              'bert/encoder/layer_1/attention/output/dense/kernel'),\n",
       "             ('bert/encoder/layer_1/attention/self/key/bias',\n",
       "              'bert/encoder/layer_1/attention/self/key/bias'),\n",
       "             ('bert/encoder/layer_1/attention/self/key/kernel',\n",
       "              'bert/encoder/layer_1/attention/self/key/kernel'),\n",
       "             ('bert/encoder/layer_1/attention/self/query/bias',\n",
       "              'bert/encoder/layer_1/attention/self/query/bias'),\n",
       "             ('bert/encoder/layer_1/attention/self/query/kernel',\n",
       "              'bert/encoder/layer_1/attention/self/query/kernel'),\n",
       "             ('bert/encoder/layer_1/attention/self/value/bias',\n",
       "              'bert/encoder/layer_1/attention/self/value/bias'),\n",
       "             ('bert/encoder/layer_1/attention/self/value/kernel',\n",
       "              'bert/encoder/layer_1/attention/self/value/kernel'),\n",
       "             ('bert/encoder/layer_1/intermediate/dense/bias',\n",
       "              'bert/encoder/layer_1/intermediate/dense/bias'),\n",
       "             ('bert/encoder/layer_1/intermediate/dense/kernel',\n",
       "              'bert/encoder/layer_1/intermediate/dense/kernel'),\n",
       "             ('bert/encoder/layer_1/output/dense/bias',\n",
       "              'bert/encoder/layer_1/output/dense/bias'),\n",
       "             ('bert/encoder/layer_1/output/dense/kernel',\n",
       "              'bert/encoder/layer_1/output/dense/kernel'),\n",
       "             ('bert/encoder/layer_2/attention/output/dense/bias',\n",
       "              'bert/encoder/layer_2/attention/output/dense/bias'),\n",
       "             ('bert/encoder/layer_2/attention/output/dense/kernel',\n",
       "              'bert/encoder/layer_2/attention/output/dense/kernel'),\n",
       "             ('bert/encoder/layer_2/attention/self/key/bias',\n",
       "              'bert/encoder/layer_2/attention/self/key/bias'),\n",
       "             ('bert/encoder/layer_2/attention/self/key/kernel',\n",
       "              'bert/encoder/layer_2/attention/self/key/kernel'),\n",
       "             ('bert/encoder/layer_2/attention/self/query/bias',\n",
       "              'bert/encoder/layer_2/attention/self/query/bias'),\n",
       "             ('bert/encoder/layer_2/attention/self/query/kernel',\n",
       "              'bert/encoder/layer_2/attention/self/query/kernel'),\n",
       "             ('bert/encoder/layer_2/attention/self/value/bias',\n",
       "              'bert/encoder/layer_2/attention/self/value/bias'),\n",
       "             ('bert/encoder/layer_2/attention/self/value/kernel',\n",
       "              'bert/encoder/layer_2/attention/self/value/kernel'),\n",
       "             ('bert/encoder/layer_2/intermediate/dense/bias',\n",
       "              'bert/encoder/layer_2/intermediate/dense/bias'),\n",
       "             ('bert/encoder/layer_2/intermediate/dense/kernel',\n",
       "              'bert/encoder/layer_2/intermediate/dense/kernel'),\n",
       "             ('bert/encoder/layer_2/output/dense/bias',\n",
       "              'bert/encoder/layer_2/output/dense/bias'),\n",
       "             ('bert/encoder/layer_2/output/dense/kernel',\n",
       "              'bert/encoder/layer_2/output/dense/kernel'),\n",
       "             ('bert/pooler/dense/bias', 'bert/pooler/dense/bias'),\n",
       "             ('bert/pooler/dense/kernel', 'bert/pooler/dense/kernel'),\n",
       "             ('bert/embeddings/LayerNorm/', 'bert/embeddings/LayerNorm/'),\n",
       "             ('bert/encoder/layer_0/attention/output/LayerNorm/',\n",
       "              'bert/encoder/layer_0/attention/output/LayerNorm/'),\n",
       "             ('bert/encoder/layer_0/output/LayerNorm/',\n",
       "              'bert/encoder/layer_0/output/LayerNorm/'),\n",
       "             ('bert/encoder/layer_1/attention/output/LayerNorm/',\n",
       "              'bert/encoder/layer_1/attention/output/LayerNorm/'),\n",
       "             ('bert/encoder/layer_1/output/LayerNorm/',\n",
       "              'bert/encoder/layer_1/output/LayerNorm/'),\n",
       "             ('bert/encoder/layer_2/attention/output/LayerNorm/',\n",
       "              'bert/encoder/layer_2/attention/output/LayerNorm/'),\n",
       "             ('bert/encoder/layer_2/output/LayerNorm/',\n",
       "              'bert/encoder/layer_2/output/LayerNorm/')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/self/query/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/self/query/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/self/key/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/self/key/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/self/value/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/self/value/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/output/dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/output/dense/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/output/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_0/attention/output/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_0/intermediate/dense/kernel:0' shape=(1024, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/intermediate/dense/bias:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/output/dense/kernel:0' shape=(4096, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/output/dense/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_0/output/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_0/output/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/self/query/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/self/query/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/self/key/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/self/key/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/self/value/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/self/value/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/output/dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/output/dense/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/output/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_1/attention/output/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_1/intermediate/dense/kernel:0' shape=(1024, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/intermediate/dense/bias:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/output/dense/kernel:0' shape=(4096, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/output/dense/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_1/output/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_1/output/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/self/query/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/self/query/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/self/key/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/self/key/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/self/value/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/self/value/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/output/dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/output/dense/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/output/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_2/attention/output/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_2/intermediate/dense/kernel:0' shape=(1024, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/intermediate/dense/bias:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/output/dense/kernel:0' shape=(4096, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/output/dense/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/encoder/layer_2/output/LayerNorm/gamma:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/encoder/layer_2/output/LayerNorm/beta:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'bert/pooler/dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'bert/pooler/dense/bias:0' shape=(1024,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bert/embeddings/LayerNorm/beta', [1024]),\n",
       " ('bert/embeddings/LayerNorm/gamma', [1024]),\n",
       " ('bert/embeddings/position_embeddings', [512, 1024]),\n",
       " ('bert/embeddings/token_type_embeddings', [2, 1024]),\n",
       " ('bert/embeddings/word_embeddings', [21128, 1024]),\n",
       " ('bert/encoder/layer_0/attention/output/LayerNorm/beta', [1024]),\n",
       " ('bert/encoder/layer_0/attention/output/LayerNorm/gamma', [1024]),\n",
       " ('bert/encoder/layer_0/attention/output/dense/bias', [1024]),\n",
       " ('bert/encoder/layer_0/attention/output/dense/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_0/attention/self/key/bias', [1024]),\n",
       " ('bert/encoder/layer_0/attention/self/key/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_0/attention/self/query/bias', [1024]),\n",
       " ('bert/encoder/layer_0/attention/self/query/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_0/attention/self/value/bias', [1024]),\n",
       " ('bert/encoder/layer_0/attention/self/value/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_0/intermediate/dense/bias', [4096]),\n",
       " ('bert/encoder/layer_0/intermediate/dense/kernel', [1024, 4096]),\n",
       " ('bert/encoder/layer_0/output/LayerNorm/beta', [1024]),\n",
       " ('bert/encoder/layer_0/output/LayerNorm/gamma', [1024]),\n",
       " ('bert/encoder/layer_0/output/dense/bias', [1024]),\n",
       " ('bert/encoder/layer_0/output/dense/kernel', [4096, 1024]),\n",
       " ('bert/encoder/layer_1/attention/output/LayerNorm/beta', [1024]),\n",
       " ('bert/encoder/layer_1/attention/output/LayerNorm/gamma', [1024]),\n",
       " ('bert/encoder/layer_1/attention/output/dense/bias', [1024]),\n",
       " ('bert/encoder/layer_1/attention/output/dense/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_1/attention/self/key/bias', [1024]),\n",
       " ('bert/encoder/layer_1/attention/self/key/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_1/attention/self/query/bias', [1024]),\n",
       " ('bert/encoder/layer_1/attention/self/query/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_1/attention/self/value/bias', [1024]),\n",
       " ('bert/encoder/layer_1/attention/self/value/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_1/intermediate/dense/bias', [4096]),\n",
       " ('bert/encoder/layer_1/intermediate/dense/kernel', [1024, 4096]),\n",
       " ('bert/encoder/layer_1/output/LayerNorm/beta', [1024]),\n",
       " ('bert/encoder/layer_1/output/LayerNorm/gamma', [1024]),\n",
       " ('bert/encoder/layer_1/output/dense/bias', [1024]),\n",
       " ('bert/encoder/layer_1/output/dense/kernel', [4096, 1024]),\n",
       " ('bert/encoder/layer_2/attention/output/LayerNorm/beta', [1024]),\n",
       " ('bert/encoder/layer_2/attention/output/LayerNorm/gamma', [1024]),\n",
       " ('bert/encoder/layer_2/attention/output/dense/bias', [1024]),\n",
       " ('bert/encoder/layer_2/attention/output/dense/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_2/attention/self/key/bias', [1024]),\n",
       " ('bert/encoder/layer_2/attention/self/key/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_2/attention/self/query/bias', [1024]),\n",
       " ('bert/encoder/layer_2/attention/self/query/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_2/attention/self/value/bias', [1024]),\n",
       " ('bert/encoder/layer_2/attention/self/value/kernel', [1024, 1024]),\n",
       " ('bert/encoder/layer_2/intermediate/dense/bias', [4096]),\n",
       " ('bert/encoder/layer_2/intermediate/dense/kernel', [1024, 4096]),\n",
       " ('bert/encoder/layer_2/output/LayerNorm/beta', [1024]),\n",
       " ('bert/encoder/layer_2/output/LayerNorm/gamma', [1024]),\n",
       " ('bert/encoder/layer_2/output/dense/bias', [1024]),\n",
       " ('bert/encoder/layer_2/output/dense/kernel', [4096, 1024]),\n",
       " ('bert/pooler/dense/bias', [1024]),\n",
       " ('bert/pooler/dense/kernel', [1024, 1024]),\n",
       " ('cls/predictions/key/bias', [1024]),\n",
       " ('cls/predictions/key/kernel', [2048, 1024]),\n",
       " ('cls/predictions/output_bias', [21128]),\n",
       " ('cls/predictions/output_prev_bias', [21128]),\n",
       " ('cls/predictions/query/bias', [1024]),\n",
       " ('cls/predictions/query/kernel', [2048, 1024]),\n",
       " ('cls/predictions/transform/LayerNorm/beta', [1024]),\n",
       " ('cls/predictions/transform/LayerNorm/gamma', [1024]),\n",
       " ('cls/predictions/transform/dense/bias', [1024]),\n",
       " ('cls/predictions/transform/dense/kernel', [1024, 1024]),\n",
       " ('cls/predictions/value/bias', [1024]),\n",
       " ('cls/predictions/value/kernel', [2048, 1024]),\n",
       " ('cls/seq_relationship/output_bias', [2]),\n",
       " ('cls/seq_relationship/output_weights', [2, 1024]),\n",
       " ('global_step', [])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.list_variables(init_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERT inference\n",
    "\n",
    "# def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "#   \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
